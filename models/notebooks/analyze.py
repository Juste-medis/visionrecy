# from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
# import torch
# from PIL import Image
# import requests
# import time

# # DÃ©sactiver CUDA et forcer l'utilisation du CPU
# torch.cuda.is_available = lambda: False


# def analyze_image_with_llava(image_path, question="What is shown in this image?"):
#     """
#     Analyse une image avec LLaVA sur CPU
#     """
#     print("ðŸ”„ Chargement du modÃ¨le LLaVA...")
#     start_time = time.time()

#     try:
#         # Charger le processeur et le modÃ¨le (version CPU)
#         processor = LlavaNextProcessor.from_pretrained(
#             "llava-hf/llava-v1.6-mistral-7b-hf"
#         )

#         model = LlavaNextForConditionalGeneration.from_pretrained(
#             "llava-hf/llava-v1.6-mistral-7b-hf",
#             torch_dtype=torch.float32,  # Utiliser float32 au lieu de float16 pour CPU
#             low_cpu_mem_usage=True,
#             device_map="cpu"  # Forcer l'utilisation du CPU
#         )

#         load_time = time.time() - start_time
#         print(f"âœ… ModÃ¨le chargÃ© en {load_time:.2f} secondes")

#         # Charger l'image
#         print("ðŸ“¸ Chargement de l'image...")
#         if image_path.startswith('http'):
#             image = Image.open(requests.get(image_path, stream=True).raw)
#         else:
#             image = Image.open(image_path)

#         # PrÃ©parer la conversation
#         conversation = [
#             {
#                 "role": "user",
#                 "content": [
#                     {"type": "text", "text": question},
#                     {"type": "image"},
#                 ],
#             },
#         ]

#         # Formater le prompt
#         prompt = processor.apply_chat_template(
#             conversation, add_generation_prompt=True
#         )

#         # PrÃ©parer les inputs
#         inputs = processor(
#             images=image,
#             text=prompt,
#             return_tensors="pt"
#         )

#         # GÃ©nÃ©ration avec le modÃ¨le
#         print("ðŸ” Analyse de l'image en cours...")
#         generation_start = time.time()

#         output = model.generate(
#             **inputs,
#             max_new_tokens=150,  # RÃ©duire un peu pour CPU
#             do_sample=True,
#             temperature=0.2,
#             top_p=0.95,
#             pad_token_id=processor.tokenizer.eos_token_id
#         )

#         generation_time = time.time() - generation_start
#         print(f"âœ… Analyse terminÃ©e en {generation_time:.2f} secondes")

#         # DÃ©coder la rÃ©ponse
#         response = processor.decode(output[0], skip_special_tokens=True)

#         # Nettoyer la rÃ©ponse (enlever le prompt)
#         if "ASSISTANT:" in response:
#             response = response.split("ASSISTANT:")[-1].strip()

#         total_time = time.time() - start_time
#         print(f"â±ï¸ Temps total: {total_time:.2f} secondes")

#         return response

#     except Exception as e:
#         print(f"âŒ Erreur: {e}")
#         return None

# # Version optimisÃ©e avec gestion de mÃ©moire


# def analyze_image_optimized(image_path, question="What is shown in this image?"):
#     """
#     Version optimisÃ©e pour CPU avec gestion de mÃ©moire
#     """
#     print("ðŸ”„ Chargement du modÃ¨le LLaVA (version optimisÃ©e)...")

#     try:
#         # Charger seulement ce dont on a besoin
#         from transformers import AutoProcessor, AutoModelForVision2Seq

#         processor = AutoProcessor.from_pretrained(
#             "llava-hf/llava-v1.6-mistral-7b-hf"
#         )

#         model = AutoModelForVision2Seq.from_pretrained(
#             "llava-hf/llava-v1.6-mistral-7b-hf",
#             torch_dtype=torch.float32,
#             device_map="cpu",
#             low_cpu_mem_usage=True
#         )

#         # Charger et prÃ©traiter l'image
#         if image_path.startswith('http'):
#             image = Image.open(requests.get(image_path, stream=True).raw)
#         else:
#             image = Image.open(image_path)

#         # PrÃ©parer les inputs
#         inputs = processor(
#             text=question,
#             images=image,
#             return_tensors="pt"
#         )

#         # GÃ©nÃ©ration
#         print("ðŸ” Analyse en cours...")
#         with torch.no_grad():
#             output = model.generate(
#                 **inputs,
#                 max_new_tokens=100,
#                 do_sample=True,
#                 temperature=0.1  # TempÃ©rature plus basse pour plus de cohÃ©rence
#             )

#         # DÃ©coder
#         response = processor.decode(output[0], skip_special_tokens=True)
#         return response

#     except Exception as e:
#         print(f"âŒ Erreur: {e}")
#         return None
#  # Version avec gestion de mÃ©moire explicite


# def analyze_image_with_memory_management(image_path):
#     """
#     Version avec gestion manuelle de la mÃ©moire pour Ã©viter les fuites
#     """
#     import gc

#     try:
#         # Nettoyer la mÃ©moire avant de commencer
#         gc.collect()
#         torch.cuda.empty_cache() if torch.cuda.is_available() else None

#         processor = LlavaNextProcessor.from_pretrained(
#             "llava-hf/llava-v1.6-mistral-7b-hf"
#         )

#         model = LlavaNextForConditionalGeneration.from_pretrained(
#             "llava-hf/llava-v1.6-mistral-7b-hf",
#             torch_dtype=torch.float32,
#             device_map="cpu"
#         )

#         # Charger l'image
#         image = Image.open(image_path) if not image_path.startswith('http') else \
#             Image.open(requests.get(image_path, stream=True).raw)

#         # PrÃ©parer la conversation
#         conversation = [
#             {
#                 "role": "user",
#                 "content": [
#                     {"type": "text", "text": "DÃ©cris cette image en dÃ©tail en franÃ§ais."},
#                     {"type": "image"},
#                 ],
#             },
#         ]

#         prompt = processor.apply_chat_template(
#             conversation, add_generation_prompt=True)
#         inputs = processor(images=image, text=prompt, return_tensors="pt")

#         # GÃ©nÃ©ration
#         output = model.generate(**inputs, max_new_tokens=200)
#         response = processor.decode(output[0], skip_special_tokens=True)

#         # Nettoyer
#         del processor, model, inputs, output
#         gc.collect()

#         return response

#     except Exception as e:
#         print(f"Erreur: {e}")
#         return None


# # Exemple d'utilisation
# if __name__ == "__main__":
#     # Test avec une image locale
#     image_path = "https://jataietatdeslieu.adidome.com/api/uploads/IMG_6544.jpg"

#     print("ðŸ¤– DÃ©marrage de l'analyse LLaVA sur CPU...")
#     result = analyze_image_with_llava(
#         image_path,
#         """Analyse cette image d'une piÃ¨ce et fournis une rÃ©ponse structurÃ©e en JSON en franÃ§ais.
#     Pour chaque Ã©lÃ©ment visible, indique:
#     - Ã©lÃ©ment: le type d'Ã©lÃ©ment (mur, sol, plafond, fenÃªtre, porte, meuble, Ã©quipement)
#     - couleur: la couleur principale
#     - Ã©tat: excellent, bon, moyen, mauvais, trÃ¨s mauvais
#     - matÃ©riau: si identifiable
#     - remarques: observations supplÃ©mentaires

#     RÃ©ponds UNIQUEMENT avec du JSON valide sans texte supplÃ©mentaire. Format:
#     {
#         "analyse_globale": "description gÃ©nÃ©rale",
#         "elements": [
#             {
#                 "element": "type",
#                 "couleur": "couleur",
#                 "etat": "Ã©tat",
#                 "materiau": "matÃ©riau",
#                 "remarques": "observations"
#             }
#         ],
#         "resume": {
#             "nombre_elements": 0,
#             "etat_moyen": "moyen"
#         }
#     }"""
#     )

#     if result:
#         print("\nðŸ“‹ RÃ‰SULTAT DE L'ANALYSE:")
#         print("=" * 50)
#         print(result)
#         print("=" * 50)
#     else:
#         print("âŒ L'analyse a Ã©chouÃ©")


import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

processor = BlipProcessor.from_pretrained(
    "Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-base")

img_url = 'https://jataietatdeslieu.adidome.com/api/uploads/IMG_6544.jpg'
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

# conditional image captioning.
text = "a photography of"
inputs = processor(raw_image, text, return_tensors="pt")

out = model.generate(**inputs)
print(processor.decode(out[0], skip_special_tokens=True))
# >>> a photography of a woman and her dog

# unconditional image captioning
inputs = processor(raw_image, return_tensors="pt")

out = model.generate(**inputs)
print(processor.decode(out[0], skip_special_tokens=True))
